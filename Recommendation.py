# -*- coding: utf-8 -*-
"""Baharuddin Nur Maulana_Submission Model  Sistem Rekomendasi

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PJX04MxVPLV3ayxoQqEZfbbNsrkTpRWi

#**Proyek Model Sistem Rekomendasi : Sistem Rekomendasi Anime**

by : Baharuddin Nur Maulana

Import Library
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import zipfile
import os
import re
import warnings
warnings.filterwarnings('ignore')

"""#**CONTENT BASED FILTERING**

##**Data Understanding**

Import Dataset
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d CooperUnion/anime-recommendations-database

!unzip anime-recommendations-database.zip

df_anime = pd.read_csv('/content/anime.csv')

"""Melakukan pengecekan missing value dengan fungsi isnull()"""

df_anime.isnull().sum()

"""Dari informasi diatas, menunjukkan terjadinya missing value pada beberapa kolom  dataset diatas diantaranya pada kolom genre, type dan rating."""

df_anime.info()

df_anime.describe()

df_anime

"""Dari hasil dataset yang dihasilkan diatas, kita dapat mengetahui bahwa dataset tersebut memiliki 12294 baris dan 7 kolom

Visualisasi Data
"""

type_count = df_anime['type'].value_counts()
sns.barplot(x=type_count.index,
            y=type_count.values, 
            palette='bright').set_title('Tipe Anime')
plt.tight_layout()
plt.show()

"""Pada hasil grafik diatas, menunjukkan bahwa tipe anime di Tv lebih tinggi(> 3500) dibandingkan tipe anime yang lain, dan music menunjukkan tipe anime paling rendah (< 500).

##**Data Preparation**

Melakukan penangan missing value pada kolom genre, type dan rating menggunakan fungsi dropna()
"""

df_anime.dropna(inplace = True)

df_anime.isnull().sum()

df_anime.head()

df_anime['rating'].value_counts()

df_anime['type'].value_counts()

df_anime['genre'].value_counts()

"""###Text Cleaning

Menghilangkan simbol/karakter yang tidak diperlukan menggunakan fungsi text cleaning.
"""

def text_cleaning(teks):
    teks = re.sub(r'&amp;', 'and', teks)
    teks = re.sub(r'I&3039;', 'I\'', teks)
    teks = re.sub(r'&quot;', '', teks)
    teks = re.sub(r'.hack//', '', teks)
    teks = re.sub(r'&#039;', '', teks)
    teks = re.sub(r'A&#039;s', '', teks)
    teks = teks.replace('...', '')
    teks = teks.replace(':', '')
    teks = teks.replace('None', '')
    
    return teks
df_anime['name'] = df_anime['name'].apply(text_cleaning)

df_anime['name']

df_anime['name'].unique()

"""#**Modelling & Result**"""

#Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

#Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(df_anime['genre'])

#Melihat ukuran matrix tfidf
tfidf_matrix.shape

#Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

from sklearn.metrics.pairwise import cosine_similarity

#Menghiitung cosine similitary pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

#Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_anime['name'], columns=df_anime['name'])

#Melihat similarity matrix pada setiap anime
cosine_sim_df.sample(10, axis=1).sample(10, axis=0)

"""#**Evaluation**"""

def get_recommendations(name, similarity_data=cosine_sim_df, items=df_anime[['name', 'genre', 'rating', 'type']], k=8):
  
  #Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
  #Dataframe diubah menjadi numpy
  #Range(start, stop, step)
  index = similarity_data.loc[:,name].to_numpy().argpartition(
      range(-1, -k, -1))
  
  #Mengambil data dengan similarity terbesar dari index yang ada
  closest = similarity_data.columns[index[-1:-(k+2):-1]]
  
  #Drop name agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
  closest = closest.drop(name, errors='ignore')
  return pd.DataFrame(closest).merge(items).head(k)

df_anime[df_anime['name'].eq('Naruto')]

"""Hasil yang ditampilkan diatas, menunjukkan anime Naruto yang direkomendasikan."""

#Mendapatkan rekomendasi anime naruto
get_recommendations('Naruto')

"""Dari hasil yang ditampilkan diatas, kita dapat mengetahui 8 anime teratas yang direkomendasikan."""

fitur = df_anime[df_anime['name'] == 'Naruto']

get_fitur_genre=[]
for i in range(len(fitur.genre)):
  for x in fitur.genre.str.split(','):
    if x not in get_fitur_genre:
      get_fitur_genre.append(x)

get_fitur_genre

"""Melakukan pengecekan keakuratan sistem rekomendasi."""

fitur_recommendations = get_recommendations('Naruto')
for i in get_fitur_genre[0]:
  print(i + ": " + str((
      (fitur_recommendations['genre'].str.contains(i).count()/fitur_recommendations['genre'].count())*100)
  ))

"""Menggunakan fungsi precision untuk mengevaluasi sistem rekomendasi yang dihasilkan"""

k = 8
threshold = 5
name_ratings = fitur_recommendations['rating'].values
name_relevances = name_ratings > threshold
precision = len(name_ratings[name_relevances]) / k
print(f'The precision of the recommendation system is {precision:.1%}')

"""#**COLLABORATIVE FILTERING**

#**Data Understanding**
"""

animes = pd.read_csv('/content/anime.csv')
ratings = pd.read_csv('/content/rating.csv')

"""**Anime**"""

animes.info()

animes

"""**Rating**"""

ratings.info()

ratings

"""Dari dataset diatas, kita dapat mengetahui bahwa dataset diatas memiliki 7813737 rows dan 3 kolom."""

ratings.describe()

"""#**Data Preparation**

Melakukan pengecekan missing value dengan menampilkan grafik rating
"""

type_count = ratings['rating'].value_counts()
sns.barplot(x=type_count.index,
            y=type_count.values,
            palette='bright').set_title('Total Rating')
plt.tight_layout()
plt.show()

"""Pada hasil grafik diatas, menunjukkan bahwa terdapat ketidakseimbangan data yang dihasilkan, dimana pada data diatas menunjukan rating -1 yang sangat tinggi, sehingga diperlukan proses penanganan missing value.

###Handling Imbalanced Data

Melakukan penanganan ketidakseimbangan data, dengan cara menghapus rating yang bernilai -1.
"""

ratings.drop(ratings[ratings['rating'] == -1].index, inplace=True)

ratings.shape

ratings

"""Melakukan pengecekan ulang mengenai missing value pada rating -1."""

type_count =  ratings['rating'].value_counts()
sns.barplot(x=type_count.index,
            y=type_count.values,
            palette='bright').set_title('Total Rating')
plt.tight_layout()
plt.show()

"""melakukan persiapan data untuk menyandikan (encode) fitur ‘user’ dan ‘anime_id’"""

id_user = ratings['user_id'].unique().tolist()
print('list UserID: ', id_user)

user_to_user_encoded = {x: i for i, x in enumerate(id_user)}
print('encoded userID: ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(id_user)}
print('encoded angka ke userID: ', user_encoded_to_user)

#Mengubah anime_id menjadi list tanpa nilai yang sama
anime_ids = ratings['anime_id'].unique().tolist()

#Melakukan proses encoding anime_id
animeID_to_animeID_encoded = {x: i for i, x in enumerate(anime_ids)}

#Melakukan proses encoding angka ke anime_id
animeID_encoded_to_animeID = {i: x for i, x in enumerate(anime_ids)}

#Mapping userID ke dataframe user
ratings['user'] = ratings['user_id'].map(user_to_user_encoded)

#Mapping userID ke dataframe user
ratings['name'] = ratings['anime_id'].map(animeID_to_animeID_encoded)

ratings

ratings.info()

#Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

#Mendapatkan jumlah anime_id
num_anime = len(animeID_encoded_to_animeID)
print(num_anime)

#Mengubah rating menjadi nilai float
ratings['rating'] = ratings['rating'].values.astype(np.float32)

#Nilai minimum rating
min_rating = min(ratings['rating'])

#Nilai maksimal rating
max_rating = max(ratings['rating'])

print('Number of User: {}, Number of Anime: {}, Min Rating; {}, Max Rating: {}'. format(num_users, num_anime, min_rating, max_rating))

"""###Membagi Data untuk Training dan Validasi"""

#Mengacak dataset
ratings = ratings.sample(frac=1, random_state=42)
ratings

"""membagi data train dan validasi dengan komposisi 80:20. Namun sebelumnya, kita perlu memetakan (mapping) data user dan anime menjadi satu value terlebih dahulu. Lalu, membuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

x = ratings[['user', 'name']].values

#Membuat variabel y untuk membuat rating dari hasil
y = ratings['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

#Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
print(x, y)

"""#**Modelling**

###Melakukan Proses Training
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embeddings resto
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2) 
 
    x = dot_user_anime + user_bias + anime_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""#**Evaluation**

###Visualisasi Metrik
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

anime_df = animes
ratings = pd.read_csv('rating.csv')
 
# Mengambil sample user
user_id = ratings.user_id.sample(1).iloc[0]
anime_visited_by_user = ratings[ratings.user_id == user_id]
 

anime_not_visited = anime_df[~anime_df['anime_id'].isin(anime_visited_by_user.user_id.values)]['anime_id'] 
anime_not_visited = list(
    set(anime_not_visited)
    .intersection(set(animeID_to_animeID_encoded.keys()))
)
 
anime_not_visited = [[animeID_to_animeID_encoded.get(x)] for x in anime_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_visited), anime_not_visited)
)

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    animeID_encoded_to_animeID.get(anime_not_visited[x][0]) for x in top_ratings_indices

]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('---' * 8)

top_anime_user = (
    anime_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values

)

anime_df_rows = anime_df[anime_df['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
  print(row.name, ':', row.genre)

  print('----' * 8)
  print('Top 10 Anime Recommendation')
  print('----' * 8)

  recommended_anime = anime_df[anime_df['anime_id'].isin(recommended_anime_ids)]
  for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)

"""Dari output tersebut, kita dapat membandingkan antara Anime with high ratings from user dan Top 10 Anime recommendation untuk user."""